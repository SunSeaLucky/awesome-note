---
title: Transformer LM 资源消耗问题
date: 2025-12-06 00:25:00 +0800
math: true
mermaid: true
categories: [数学]
tags: [math]
---

最近在学习 Standford 336 的课程，由于本人对这类计算问题不甚熟悉，因此记录一下。下图是课程中的 Transformer 结构：

![](/assets/img/2025-12-06-train/2025-12-06-13-01-43.png)

## 问题一

考虑 GPT-2 XL，配置如下：

```
vocab_size : 50,257
context_length : 1,024
num_layers : 48
d_model : 1,600
num_heads : 25
d_ff : 6,400
```

假设我们使用这样的配置在上述结构中：

**1）模型中有多少可训练的参数？**

- Embedding

其参数的尺寸是 $num\_embeddings\times embedding\_dim$，所以

$$num\_embeddings\times embedding\_dim=vocab\_size\times d\_model=50,257\times 1,600=80,411,200$$

- RMSNorm

因为每个 TransformerBlock 中都有 RMSNorm、CausalMultiHeadAttention、RotaryPositionalEmbedding、SwiGLU，所以我们先讨论这三块。RMSNorm 的参数数量是 $d\_model$，即

$$d\_model=1,600$$

- RotaryPositionalEmbedding

虽然 RoPE 并没有需要训练的参数，但其在计算时也会占用内存，且大小为

$$seq\_len \times \frac{d_k}{2} \times 2$$

其中 $d_k=d_v=d\_model/num\_heads=64$，故 RoPE 的（最大）参数量为：

$$seq\_len \times \frac{d_k}{2} \times 2=context\_length\times\frac{d_k}{2}\times 2=65,536$$

- SwiGLU

```python
class SwiGLU(torch.nn.Module):
    def __init__(self, d_model: int, d_ff: int = None, device=None, dtype=None):
        super().__init__()
        if d_ff is None:
            d_ff = 8 * d_model / 3
        self.w1 = Linear(d_model, d_ff, device, dtype)
        self.w2 = Linear(d_ff, d_model, device, dtype)
        self.w3 = Linear(d_model, d_ff, device, dtype)

    def forward(self, x: torch.Tensor):  # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]
        _x = self.w1(x)
        x_silu = _x * torch.sigmoid(_x)
        return self.w2(self.w3(x) * x_silu)
```

容易看出，总参数量为：

$$3\times d\_model\times d\_ff = 3 \times 1,600 \times 6,400=30,720,000$$

- CausalMultiHeadAttention

其 $Q、K、V、O$ 均为 $d\_model\times d\_model$，其中 $O$ 是上投影层。总共可训练参数量为：

$$4\times d\_model\times d\_model=4\times 1,600\times 1,600=10,240,000$$

- Transformer

每个 TransformerBlock 包含 2 个 RMSNorm，1 个 CausalMultiHeadAttention，1 个 SwiGLU。故总参数量为：

$$2\times1,600+10,240,000+30,720,000=40,963,200$$

Transformer 包括 1 个 Embedding、1 个 RotaryPositionalEmbedding（不计入可训练参数）、48 个 TransformerBlock、1 个 RMSNorm、1 个 Linear 上投影层（$d\_model\times vocab\_size$）。故总参数量为：

$$
1\times 80,411,200 + 48\times 40,963,200 + 1\times 1,600 + 1\times 1,600\times 50,257=2,127,057,600
$$

**2）假设每个参数都是单精度浮点数，载入这个模型需要多少内存？**

每个单精度浮点数占 4 字节，故总内存为：

$$
4\times 2,127,057,600=8,508,230,400~bytes \approx 7.9239~GB
$$